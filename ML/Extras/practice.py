# -*- coding: utf-8 -*-
"""Practice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hUpOg-EwpesJMOwbOoAeMBRggdh2GMdw

# Uber Price Detection
"""



"""# Email Classification"""



"""# Neural Network Classifier"""



"""# Gradient Descent Algorithm

Implement Gradient Descent Algorithm to find the local minima of a function.
For example, find the local minima of the function y=(x+3)² starting from the point x=2.
"""

# Function to calculate the derivative of the given function
def derivative(x):
    return 2 * (x + 3)

# Gradient Descent Algorithm
def gradient_descent(learning_rate, precision, max_iterations, start_point):
    x = start_point
    for i in range(max_iterations):
        gradient = derivative(x)
        x = x - learning_rate * gradient

        # Stopping criterion: stop when gradient is very small (close to 0)
        if abs(gradient) < precision:
            break

    return x

# Parameters for Gradient Descent
learning_rate = 0.01  # Step size or learning rate
precision = 0.0001    # Desired precision of result
max_iterations = 1000 # Maximum number of iterations
start_point = 2       # Starting point for x

# Run Gradient Descent
local_minima = gradient_descent(learning_rate, precision, max_iterations, start_point)
print(f"Local minimum at x = {local_minima}")

"""Implement Gradient Descent Algorithm to find the local minima of a function. For example, find the local minima of the function y=(x+3)² starting from the point x=2."""

import sympy as smp
import matplotlib.pyplot as plt
import numpy as np

def expression_fn(string_expression):
  return smp.sympify(string_expression)

def evaluate(string_expression, value):
  # Define a symbolic variable x
  x = smp.Symbol('x')

  # Convert string expression to a SymPy expression
  expression = expression_fn(string_expression)

  # Evaluate the SymPy expression for a specific 'value' of 'x'
  result = smp.simplify(expression.subs(x, value))
  return result

# Define a symbolic variable x
x = smp.Symbol('x')

expression = ""
derivative = ""

# Expression as Input
expression = input("Enter The Expression : ")

# Calculate the derivative of the function with respect to x
der_obj = smp.Derivative(expression, x)
derivative = str(der_obj.doit())

inp = int(input("At which point you want to start ? : "))
exp_val = evaluate(expression,inp)
der_val = evaluate(derivative,inp) # other way to evaluate : der_obj.doit().subs(x, 2)

# Generate 100 x-values between -15 and 15 for plotting
space = np.linspace(-15, 15, 100)
vals = list()

for i in space:
  vals.append(evaluate(expression,i))

# Plot the function y = (x+3)^2 using the x1 values
plt.plot(space,vals,label='Function: '+expression)

# # Plot a marker at the point x=2, y=(x+3)^2 at x=2
plt.plot(inp,exp_val,marker='o', markersize=8, label='Point at x='+str(inp))
plt.annotate(f'Point: ({inp}, {exp_val})', (inp, exp_val), textcoords="offset points", xytext=(5,5), ha='center')

# Add labels, title, and legend to the plot
plt.xlabel('x')
plt.ylabel('y')
plt.title('Gradient Descent Optimization')
plt.legend()

# Show the plot
plt.show()

def gradient(expression, derivative, start, aplha, max_iter):
    x_list=list()
    x=start
    x_list.append(x)
    for i in range(max_iter):
      grad=evaluate(derivative,x);
      x=x-(grad*alpha)
      x_list.append(x)
    return x_list

# Set starting point, alpha (learning rate), and maximum iterations for optimization
start = 2
alpha = 0.1
max_iter = 50

# Generate 100 x-values between -5 and 5 for plotting
space = np.linspace(-5, 5, 100)
vals = list()

for i in space:
  vals.append(evaluate(expression,i))

# Plot the function y = (x+3)^2 using the x1 values
plt.plot(space,vals,label='Function: '+expression)

grad_x = np.array(gradient(expression, derivative, start, alpha, max_iter))
vals = list()

for i in grad_x:
  vals.append(evaluate(expression,i))

plt.plot(grad_x,vals,marker='o')
plt.plot(grad_x[-1],evaluate(expression,grad_x[-1]),marker='o',color='green')
plt.annotate(f'Point: ({grad_x[-1]}, {evaluate(expression,grad_x[-1])})', (grad_x[-1], evaluate(expression,grad_x[-1])), textcoords="offset points", xytext=(5,5), ha='center')

"""# Clustering"""

